tidy()
lm(price ~ bath, data = homes) %>%
tidy()
ggplot(homes, aes(x = bath, y = price)) + geom_point()
ggplot(homes, aes(x = log(bath), y = log(price))) + geom_point()
library(corrplot)
install.packages("corrplot")
install.packages("corrplot")
library(corrplot)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(mosaicData)
library(oilabs)
library(infer)
data(RailTrail)
# volume regressed on hightemp
ride_lm <- lm(volume ~ hightemp, data = RailTrail)
summary(ride_lm)
ggplot(RailTrail, aes(x = hightemp, y = volume)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
set.seed(4747)
sample1 <- RailTrail %>%
sample_n(size = 20)
sample2 <- RailTrail %>%
sample_n(size = 20)
both_samples <- bind_rows(sample1, sample2, .id = "replicate")
ggplot(both_samples, aes(x = hightemp, y = volume, color = replicate)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
many_samples <- RailTrail %>%
rep_sample_n(size = 20, reps = 100)
ggplot(many_samples, aes(x = hightemp, y = volume, group = replicate)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
many_lm <- many_samples %>%
group_by(replicate) %>%
do(lm(volume ~ hightemp, data = .) %>% tidy()) %>%
filter(term == "hightemp")
many_lm
ggplot(many_lm, aes(x = estimate)) +
geom_density()
many_samples_10 <- RailTrail %>%
rep_sample_n(size = 10, reps = 100)
# Sample size of 10
ggplot(many_samples_10, aes(x = hightemp, y = volume, group = replicate)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
many_samples_50 <- RailTrail %>%
rep_sample_n(size = 50, reps = 100)
# Sample size of 50
ggplot(many_samples_50, aes(x = hightemp, y = volume, group = replicate)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
twins <- read_csv("J:/sample_data/twins.csv")
(obs_slope <- twins %>%
lm(Foster ~ Biological, data = .) %>%
tidy() %>%
filter(term == "Biological") %>%
pull(estimate))
perm_slope <- twins %>%
specify(Foster ~ Biological) %>%
hypothesize(null = "independence") %>%
generate(reps = 500, type = "permute") %>%
calculate(stat = "slope")
ggplot(perm_slope, aes(x = stat)) +
geom_density()
perm_slope %>%
ungroup() %>%
summarize(
mean_slopes = mean(stat),
std_err_slopes = sd(stat)
)
(abs_obs_slope <- lm(Foster ~ Biological, data = twins) %>%
tidy() %>%
filter(term == "Biological") %>%
pull(estimate) %>%
abs())
perm_slope %>%
mutate(abs_perm_slope = abs(stat)) %>%
summarize(
p_value = mean(abs_perm_slope > abs_obs_slope)
)
boot_slope <- twins %>%
# Specify linear model
specify(Foster ~ Biological) %>%
# Simulate variability with bootstrapped resamples
generate(reps = 1000, type = "bootstrap") %>%
# Calculate slopes for each resample to construct sampling distribution
calculate(stat = "slope")
boot_slope %>%
summarize(
lower_bound = mean(stat) - 2 * sd(stat),
upper_bound = mean(stat) + 2 * sd(stat)
)
# Set the alpha significance level
alpha = 0.05
# Calculate the percentile cut-off for both bounds
p_lower = 0.5 * alpha
p_upper = 1 - (0.5 * alpha)
# Calculate confidence interval
boot_slope %>%
summarize(
lower_bound = quantile(stat, p = p_lower),
upper_bound = quantile(stat, p = p_upper)
)
(twins_perm <- twins %>%
specify(Foster ~ Biological) %>%
hypothesize(null = "independence") %>%
generate(reps = 500, type = "permute") %>%
rename(Biological_perm = Biological) %>%
group_by(replicate) %>%
do(lm(Foster ~ Biological_perm, data = .) %>% tidy()))
biological_perm <- twins_perm %>%
filter(term == "Biological_perm")
degrees_of_freedom <- nrow(twins) - 2
# Set x as the t-value
ggplot(biological_perm, aes(x = statistic)) +
# Set y as the density, not the default count
geom_histogram(aes(y = ..density..)) +
# Calculate theoretical t-distribution
stat_function(fun = dt, args = list(df = degrees_of_freedom), color = "red")
# Linear model of original twin sample
model <- lm(Foster ~ Biological, data = twins)
# Extract data for model coefficient
biological_term <- model %>%
tidy() %>%
filter(term == "Biological")
# One-sided p-value
biological_term %>%
mutate(
one_sided_p_value = p.value / 2
)
biological_term %>%
mutate(
# Calculate the new test statistic
test_statistic = (estimate - 1) / std.error,
# Calculate its one-sided p-value
one_sided_p_value_of_test_statistic = pt(q = test_statistic,
df = degrees_of_freedom),
# ... and its two-sided p-value
two_sided_p_value_of_test_statistic = one_sided_p_value_of_test_statistic * 2
)
# Calculate p-value from randomization inference
perm_slope %>%
rename(perm_slope = stat) %>%
mutate(abs_perm_slope = abs(perm_slope)) %>%
summarize(
p_value = mean(abs_perm_slope > abs_obs_slope)
)
# Set the confidence level
confidence_level <- 1 - alpha
# Set the upper bound
p_upper <- 1 - (0.5 * alpha)
# Find the critical values from the t-distribution
(critical_value <- qt(p = p_upper, df = degrees_of_freedom))
(tidied_model <- lm(Foster ~ Biological, data = twins) %>%
tidy(conf.int = TRUE, conf.level = confidence_level))
tidied_model %>%
mutate(
lower = estimate - critical_value * std.error,
upper = estimate + critical_value * std.error
)
# Get observation-level values from the model
augment(model)
# Create new model of new twins
new_twins <- data.frame(Biological = seq(70, 130, 15))
# Augment the model with the new dataset
augmented_model <- augment(model, newdata = new_twins)
# See the result
augmented_model
(predictions <- augmented_model %>%
# Calculate a confidence interval on the predicted values
mutate(
# Remember, the critical value is the t-statistic
lower_mean_prediction = .fitted - critical_value * .se.fit,
upper_mean_prediction = .fitted + critical_value * .se.fit
))
# Coefficient-level
tidy(model)
# Observation-level
augment(model)
# Model-level
glance(model)
twins_sigma <- model %>%
# Get model-level information
glance() %>%
# Pull out sigma
pull(sigma)
# Find the natural variability of the points around the prediction line
predictions2 <- predictions %>%
# Calculate the std err of the predictions
mutate(std_err_of_predictions = sqrt(twins_sigma^2 + .se.fit^2))
# Prediction interval
predictions3 <- predictions2 %>%
# Calculate the prediction intervals
mutate(
lower_response_prediction = .fitted - critical_value * std_err_of_predictions,
upper_response_prediction = .fitted + critical_value * std_err_of_predictions
)
# Visualize prediction intervals
ggplot() +
geom_point(aes(x = Biological, y = Foster), data = twins) +
geom_smooth(aes(x = Biological, y = Foster), data = twins, method = "lm") +
# Add a ribbon layer
geom_ribbon(aes(x = Biological, ymin = lower_response_prediction,
ymax = upper_response_prediction),
data = predictions3, alpha = 0.2, fill = "red")
# Plot relationship
ggplot(RailTrail, aes(x = hightemp, y = volume)) +
geom_point()
# Augment linear model
ride_lm_augmented <- augment(ride_lm)
# Plot residuals versus fitted values
ggplot(ride_lm_augmented, aes(x = .fitted, y = .resid)) +
geom_point() +
geom_hline(yintercept = 0)
homes <- read_csv("J:/sample_data/la-homes.csv")
lm(log(price) ~ log(sqft), data = homes) %>%
tidy()
lm(log(price) ~ log(bath), data = homes) %>%
tidy()
corr_homes <- homes %>% select(price, bath, sqft)
corrplot(corr_homes)
corr_homes
cor(corr_homes)
corrplot(cor(corr_homes))
corr_homes <- homes %>% select(price, bath, sqft)
corr_homes <- cor(corr_homes)
corrplot(corr_homes, type = "lower")
corr_homes <- homes %>% select(price, bath, sqft)
corr_homes <- cor(corr_homes)
corrplot(corr_homes, type = "lower")
corr_homes <- homes %>% select(price, bath, sqft)
corr_homes <- cor(corr_homes)
corrplot,mixed(corr_homes)
corr_homes <- homes %>% select(price, bath, sqft)
corr_homes <- cor(corr_homes)
corrplot.mixed(corr_homes)
lm(log(price) ~ log(sqft) + log(bath), data = homes) %>%
tidy()
nyc_rest <- read_csv("J:/sample_data/nyc_restaurant.csv")
nyc_rest <- read_csv("J:/sample_data/nyc_restaurant.csv")
lm(Price ~ Service, data = nyc_rest) %>%
tidy()
lm(Price ~ Service + Food + Decor, data = nyc_rest) %>%
tidy()
sqrt(
(0.55*0.45) / 100
)
0.05*2.58
.55-0.129
.55+0.129
0.95-0.9495
0.95-0.9505
1-0.334
sqrt(
(0.334*0.666) / 1000
)
0.015*1.645
0.334-0.025
0.334+0.025
96/200
1-(96/200)
sqrt(
(0.48*0.52) / 200
)
SEp <- sqrt(
(0.48*0.52) / 200
)
SEp * 1.645
0.4*0.6
0.05^2
1.96^2
0.0025/3.8416
3.8416/0.0025
1536.64*0.24
SEp <- sqrt(
(0.5*0.5)/1000
)
SEp * 1.96
SEp <- sqrt(
(0.5*0.5)/4000
)
SEp * 1.96
0.9*0.1
1.96/0.02
98^2
9604*0.09
load("~/GitHub/osl-zhang-et-al-2014/src/workspace.RData")
# Set working directory
setwd("~/GitHub/osl-zhang-et-al-2014/src")
# Load dependencies
library(psych)
library(broom)
library(car)
library(nlme)
library(multcomp)
library(tidyverse)
curious_lm <- lm(curious_rating ~ condition*curious_time + Error(subject_id),
data = zhang_long)
# Create aov object
curious_aov <- aov(curious_rating ~ condition*curious_time + Error(subject_id),
data = zhang_long)
View(extraordinary)
# Split data by condition
ordinary <- zhang_long %>% filter(condition == "ordinary")
extraordinary <- zhang_long %>% filter(condition == "extraordinary")
# Simple main effects
anova(lm(extra_rating ~ extra_time, data = ordinary))
# Simple main effects
anova(aov(extra_rating ~ extra_time + Error(subject_id), data = ordinary))
# Simple main effects
Anova(aov(extra_rating ~ extra_time + Error(subject_id), data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id), data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time, data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time * subject_id, data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time + subject_id, data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time * condition, data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time, data = ordinary))
# Simple main effects
Anova(aov(extra_rating ~ extra_time, data = ordinary))
# Simple main effects
Anova(aov(extra_rating ~ extra_time + Error(subject_id), data = ordinary))
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random ~1|subject_id, data = ordinary))
# Simple main effects
lme(extra_rating ~ extra_time, random ~1|subject_id, data = ordinary)
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = ordinary), type == "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = ordinary), type = "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time*condition, random = ~1|subject_id/condition, data = zhang_long), type = "III")
# Split data by condition
ordinary <- zhang_clean %>% filter(condition == "ordinary")
extraordinary <- zhang_clean %>% filter(condition == "extraordinary")
?TukeyHSD
# Print ANOVA summary using type III sum of squares
options(contrasts = c("contr.sum", "contr.poly"))
# Simple main effects
Anova(lme(extra_rating ~ extra_time*condition, random = ~1|subject_id/condition, data = zhang_long), type = "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = zhang_long), type = "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = ordinary), type = "III")
# Split data by condition
ordinary <- zhang_long %>% filter(condition == "ordinary")
extraordinary <- zhang_long %>% filter(condition == "extraordinary")
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = ordinary), type = "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time, data = ordinary), type = "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1, data = ordinary), type = "III")
# Simple main effects
Anova(lm(extra_rating ~ extra_time, data = ordinary), type = "III")
# Simple main effects
Anova(aov(extra_rating ~ extra_time, data = ordinary), type = "III")
# Simple main effects
Anova(aov(extra_rating ~ extra_time, data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time, data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id), data = ordinary))
ordinary$extra_time
count(ordinary$extra_time)
ordinary %>% group_by(extra_time) %>% summarise(n = n())
ordinary
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id/extra_time), data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id + extra_time), data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id * extra_time), data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time * subject_id, data = ordinary))
# Simple main effects
summary(aov(extra_rating ~ extra_time + subject_id, data = ordinary))
# Create analysis of variance
extra_aov <- aov(extra_rating ~ condition*extra_time + Error(subject_id), data = zhang_long)
# Summarize the ANOVA
summary(extra_aov)
# Create linear mixed-effects model
extra_lme <- lme(extra_rating ~ condition*extra_time, random = ~1|subject_id,
data = zhang_long)
# Print ANOVA summary using type III sum of squares
options(contrasts = c("contr.sum", "contr.poly"))
Anova(extra_lme, type = "III")
# Create linear mixed-effects model
extra_lme <- lme(extra_rating ~ condition*extra_time, random = ~1|subject_id/condition,
data = zhang_long)
# Print ANOVA summary using type III sum of squares
options(contrasts = c("contr.sum", "contr.poly"))
Anova(extra_lme, type = "III")
Anova(extra_lme, type = "II")
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id/condition), data = zhang_long))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id * condition), data = zhang_long))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id + condition), data = zhang_long))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(condition), data = zhang_long))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(condition/subject_id), data = zhang_long))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id), data = zhang_long))
# Simple main effects
summary(aov(extra_rating ~ extra_time + Error(subject_id), data = ordinary))
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = ordinary), type = "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = ordinary))
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = zhang_long))
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id/condition, data = zhang_long))
# Simple main effects
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id/condition, data = zhang_long), type = "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time*condition, random = ~1|subject_id/condition, data = zhang_long), type = "III")
# Simple main effects
Anova(lme(extra_rating ~ extra_time*condition, random = ~1|subject_id/condition, data = zhang_long))
# Simple main effects
Anova(lme(extra_rating ~ extra_time*condition, random = ~1|subject_id, data = zhang_long))
# Simple main effects
Anova(lme(extra_rating ~ extra_time*condition, random = ~1|subject_id/(extra_time*condition), data = zhang_long))
# Simple main effects
Anova(lme(extra_rating ~ extra_time*condition, random = ~1|subject_id/(extra_time), data = zhang_long))
# Simple main effects
with(ordinary, pairwise.t.test(extra_rating, extra_time, paired = T))
# Simple main effects
anova(lm(extra_rating ~ extra_time, data = ordinary))
effect()
anova(extra_lme)
Anova(extra_lme, type = "III")
extra_lme <- lme(extra_rating ~ extra_time, random = ~1|subject_id,
data = ordinary)
Anova(extra_lme, type = "III")
Anova(extra_lme)
anova(extra_lme)
# Create linear mixed-effects model
extra_lme <- lme(extra_rating ~ condition*extra_time, random = ~1|subject_id,
data = zhang_long)
# Condition as factor
zhang_clean <- within(zhang_clean, {
condition <- factor(condition, labels = c("ordinary", "extraordinary"))
subject_id <- factor(subject_id)
})
# Number of participants in each condition
zhang_clean %>% count(condition)
# Convert data frame into long format
long_extra <- zhang_clean %>%
select(condition, subject_id, t1_extra, t2_extra) %>%
gather(key = extra_time, value = extra_rating, -c(condition, subject_id))
long_curious <- zhang_clean %>%
select(t1_curious, t2_curious) %>%
gather(key = curious_time, value = curious_rating)
long_interest <- zhang_clean %>%
select(t1_interest, t2_interest) %>%
gather(key = interest_time, value = interest_rating)
(zhang_long <- bind_cols(long_extra, long_curious, long_interest))
# Set time variables as factors
(zhang_long <- within(zhang_long, {
extra_time <- factor(extra_time)
curious_time <- factor(curious_time)
interest_time <- factor(interest_time)
}))
zhang_long
zhang_long[, 1:4]
Anova(extra_lme, type = "III")
# Create linear mixed-effects model
extra_lme <- lme(extra_rating ~ condition*extra_time, random = ~1|subject_id,
data = zhang_long)
# Print ANOVA summary using type III sum of squares
options(contrasts = c("contr.sum", "contr.poly"))
Anova(extra_lme, type = "III")
# Split data by condition
ordinary <- zhang_long %>% filter(condition == "ordinary")
extraordinary <- zhang_long %>% filter(condition == "extraordinary")
library(multcomp)
?testInteractions
??testInteractions
anova(lm(extra_rating ~ extra_time, data = ordinary))
anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = ordinary))
Anova(lme(extra_rating ~ extra_time, random = ~1|subject_id, data = ordinary), type = "III")
install.packages("ez")
library(ez)
ezANOVA(
dv = extra_rating,
wid = subject_id,
within = extra_time,
between = condition,
detailed = TRUE,
type = 3
)
ezANOVA(data = zhang_long,
dv = extra_rating,
wid = subject_id,
within = extra_time,
between = condition,
detailed = TRUE,
type = 3
)
